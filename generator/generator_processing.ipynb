{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ccf0026-95e2-4c8b-85cc-3b0867252f71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = [f for f in os.listdir('../data/unified/specs') if not f.endswith('.json')]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d4d0ac4-1350-47cf-9ab8-e349085f87a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in specs only: set()\n",
      "Files in imgs only: set()\n"
     ]
    }
   ],
   "source": [
    "# Compare contents of two folders\n",
    "\n",
    "import os\n",
    "\n",
    "# Specify the folder paths\n",
    "specs_folder = '../data/unified/specs'\n",
    "imgs_folder = '../data/unified/imgs'\n",
    "\n",
    "# Extract base file names from the specs folder\n",
    "specs_files = os.listdir(specs_folder)\n",
    "specs_base_names = {os.path.splitext(file)[0] for file in specs_files if file.endswith('.json')}\n",
    "\n",
    "# Extract base file names from the imgs folder\n",
    "imgs_files = os.listdir(imgs_folder)\n",
    "imgs_base_names = {os.path.splitext(file)[0] for file in imgs_files if file.endswith('.png')}\n",
    "\n",
    "# Find overlaps and differences\n",
    "overlap = specs_base_names.intersection(imgs_base_names)\n",
    "specs_only = specs_base_names.difference(imgs_base_names)\n",
    "imgs_only = imgs_base_names.difference(specs_base_names)\n",
    "\n",
    "# Report results\n",
    "# print(\"Overlap:\", overlap)\n",
    "print(\"Files in specs only:\", specs_only)\n",
    "print(\"Files in imgs only:\", imgs_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061006cf-0602-43f7-9325-7edddd8c8e5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def find_nested_field(data, field_name, field_value):\n",
    "    \"\"\"\n",
    "    Recursively searches for a field_name with the field_value in nested JSON data.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if key == field_name and value == field_value:\n",
    "                return True\n",
    "            elif isinstance(value, (dict, list)):\n",
    "                if find_nested_field(value, field_name, field_value):\n",
    "                    return True\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            if find_nested_field(item, field_name, field_value):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def list_json_files_with_nested_field(directory, field_name, field_value):\n",
    "    matching_files = []\n",
    "    \n",
    "    # Loop over all files in the specified directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            # Construct the full path to the file\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            try:\n",
    "                # Open and parse the JSON file\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    data = json.load(file)\n",
    "                    \n",
    "                    # Check if the nested field and value exist in the JSON data\n",
    "                    if find_nested_field(data, field_name, field_value):\n",
    "                        matching_files.append(filename)\n",
    "\n",
    "            except (json.JSONDecodeError, IOError) as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "    return matching_files\n",
    "\n",
    "# Example usage\n",
    "directory = '../data/unified/specs'\n",
    "field_name = 'type'\n",
    "field_value = 'vcf'\n",
    "matching_json_files = list_json_files_with_nested_field(directory, field_name, field_value)\n",
    "\n",
    "print(\"Files with the specified nested field:\")\n",
    "for filename in matching_json_files:\n",
    "    print(filename)\n",
    "print(len(matching_json_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec144e16-f4a6-4f26-a473-328dd4da6e00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files containing 'type':'vcf':\n",
      "VCF_INDELS_sw_0_7_s_1_2.json\n",
      "VCF_POINT_MUTATIONS_sw_1_2_s_1_0.json\n",
      "VCF_POINT_MUTATIONS_sw_0_7_s_1_0.json\n",
      "VCF_INDELS_sw_1_2_s_1_2.json\n",
      "VCF_INDELS_sw_1_2_s_0_7.json\n",
      "VCF_INDELS_sw_0_7_s_0_7.json\n",
      "VCF_POINT_MUTATIONS_sw_1_0_s_1_0.json\n",
      "VCF_INDELS_sw_1_0_s_1_2.json\n",
      "VCF_INDELS_sw_1_0_s_0_7.json\n",
      "VCF_POINT_MUTATIONS_sw_1_0_s_1_2.json\n",
      "VCF_INDELS_sw_1_0_s_1_0.json\n",
      "VCF_POINT_MUTATIONS_sw_1_0_s_0_7.json\n",
      "VCF_POINT_MUTATIONS_sw_1_2_s_1_2.json\n",
      "VCF_INDELS_sw_0_7_s_1_0.json\n",
      "VCF_INDELS_sw_1_2_s_1_0.json\n",
      "VCF_POINT_MUTATIONS_sw_0_7_s_1_2.json\n",
      "VCF_POINT_MUTATIONS_sw_0_7_s_0_7.json\n",
      "VCF_POINT_MUTATIONS_sw_1_2_s_0_7.json\n",
      "Total number of 'vcf' files: 18\n",
      "\n",
      "Files containing 'type':'gff':\n",
      "GFF_DEMO_sw_1_2_s_1_0.json\n",
      "GFF_DEMO_sw_0_7_s_1_0.json\n",
      "GFF_DEMO_sw_1_0_s_1_0.json\n",
      "GFF_DEMO_sw_1_0_s_0_7.json\n",
      "GFF_DEMO_sw_1_0_s_1_2.json\n",
      "GFF_DEMO_sw_0_7_s_0_7.json\n",
      "GFF_DEMO_sw_1_2_s_0_7.json\n",
      "GFF_DEMO_sw_1_2_s_1_2.json\n",
      "GFF_DEMO_sw_0_7_s_1_2.json\n",
      "Total number of 'gff' files: 9\n",
      "\n",
      "Files containing 'type':'bed':\n",
      "BED_DEMO_sw_0_7_s_0_7.json\n",
      "Total number of 'bed' files: 1\n",
      "\n",
      "Files containing 'type':'bam':\n",
      "EX_SPEC_PILEUP_sw_1_0_s_0_7.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_0_s_0_7.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_0_s_1_2.json\n",
      "EX_SPEC_PILEUP_sw_1_0_s_1_2.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_0_7_s_0_7.json\n",
      "EX_SPEC_PILEUP_sw_1_2_s_0_7.json\n",
      "EX_SPEC_PILEUP_sw_0_7_s_0_7.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_2_s_0_7.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_2_s_1_2.json\n",
      "EX_SPEC_PILEUP_sw_0_7_s_1_2.json\n",
      "EX_SPEC_PILEUP_sw_1_2_s_1_2.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_0_7_s_1_2.json\n",
      "EX_SPEC_PILEUP_sw_0_7_s_1_0.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_2_s_1_0.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_0_7_s_1_0.json\n",
      "EX_SPEC_PILEUP_sw_1_2_s_1_0.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_0_s_1_0.json\n",
      "EX_SPEC_PILEUP_sw_1_0_s_1_0.json\n",
      "Total number of 'bam' files: 18\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def find_files_with_types(folder_path, file_types):\n",
    "    matching_files = {file_type: [] for file_type in file_types}\n",
    "    \n",
    "    try:\n",
    "        # Loop through all files in the folder\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                \n",
    "                try:\n",
    "                    # Read and parse each JSON file\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        json_data = json.load(file)\n",
    "                        \n",
    "                        # Function to recursively search for specified file types in the JSON\n",
    "                        def search_for_type(obj, file_types):\n",
    "                            if isinstance(obj, dict):\n",
    "                                for key, value in obj.items():\n",
    "                                    if key == \"type\" and value in file_types:\n",
    "                                        return value\n",
    "                                    if isinstance(value, (dict, list)):\n",
    "                                        result = search_for_type(value, file_types)\n",
    "                                        if result:\n",
    "                                            return result\n",
    "                            elif isinstance(obj, list):\n",
    "                                for item in obj:\n",
    "                                    result = search_for_type(item, file_types)\n",
    "                                    if result:\n",
    "                                        return result\n",
    "                            return None\n",
    "                        \n",
    "                        # Check if any of the file types are found in the JSON\n",
    "                        file_type_found = search_for_type(json_data, file_types)\n",
    "                        if file_type_found:\n",
    "                            matching_files[file_type_found].append(filename)\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error reading JSON file {filename}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {filename}: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing folder: {e}\")\n",
    "        return {}, 0\n",
    "    \n",
    "    # Print results\n",
    "    for file_type, files in matching_files.items():\n",
    "        print(f\"\\nFiles containing 'type':'{file_type}':\")\n",
    "        for file in files:\n",
    "            print(file)\n",
    "        print(f\"Total number of '{file_type}' files: {len(files)}\")\n",
    "    \n",
    "    total_files = sum(len(files) for files in matching_files.values())\n",
    "    return matching_files, total_files\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"../data/gallery_p1/specs\"\n",
    "file_types = [\"vcf\", \"gff\", \"bed\", \"bam\"]\n",
    "files, total_count = find_files_with_types(folder_path, file_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab498d32-5ea9-4d5a-805b-e6ed1fae6d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Divide files into multiple partitions\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from math import ceil\n",
    "\n",
    "def divide_files_into_folders(source_folder, dest_folder_base, num_folders=5):\n",
    "    # Get list of files in the source folder\n",
    "    files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
    "\n",
    "    # Calculate the number of files per folder\n",
    "    num_files = len(files)\n",
    "    files_per_folder = ceil(num_files / num_folders)\n",
    "\n",
    "    # Ensure destination folders exist\n",
    "    dest_folders = []\n",
    "    for i in range(num_folders):\n",
    "        folder_name = f\"{dest_folder_base}_{i+1}\"\n",
    "        dest_folder = os.path.join(source_folder, folder_name)\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        dest_folders.append(dest_folder)\n",
    "\n",
    "    # Distribute files to each destination folder\n",
    "    for index, file in enumerate(files):\n",
    "        dest_folder = dest_folders[index // files_per_folder]\n",
    "        shutil.move(os.path.join(source_folder, file), os.path.join(dest_folder, file))\n",
    "\n",
    "# Example usage\n",
    "source_folder = '../data/unified/remaining_specs'\n",
    "dest_folder_base = 'partition'\n",
    "divide_files_into_folders(source_folder, dest_folder_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d55f6c7-4590-48cc-9b79-c05c1c29f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences found in the following files:\n",
      "DUMMY_TRACK_sw_0_7_s_0_7.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_0_7_s_0_7.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_0_7_s_1_0.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_0_7_s_1_2.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_0_s_0_7.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_0_s_1_0.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_0_s_1_2.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_2_s_0_7.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_2_s_1_0.json\n",
      "EX_SPEC_CANCER_VARIANT_PROTOTYPE_sw_1_2_s_1_2.json\n",
      "EX_SPEC_GENE_ANNOTATION_sw_0_7_s_0_7.json\n",
      "EX_SPEC_GIVE_sw_0_7_s_0_7.json\n",
      "EX_SPEC_GIVE_sw_0_7_s_1_0.json\n",
      "EX_SPEC_GIVE_sw_0_7_s_1_2.json\n",
      "EX_SPEC_GIVE_sw_1_0_s_0_7.json\n",
      "EX_SPEC_GIVE_sw_1_0_s_1_0.json\n",
      "EX_SPEC_GIVE_sw_1_0_s_1_2.json\n",
      "EX_SPEC_GIVE_sw_1_2_s_0_7.json\n",
      "EX_SPEC_GIVE_sw_1_2_s_1_0.json\n",
      "EX_SPEC_GIVE_sw_1_2_s_1_2.json\n",
      "EX_SPEC_MATRIX_HFFC6_sw_0_7_s_0_7.json\n",
      "EX_SPEC_MATRIX_HFFC6_sw_0_7_s_1_0.json\n",
      "EX_SPEC_MATRIX_HFFC6_sw_0_7_s_1_2.json\n",
      "EX_SPEC_MATRIX_HFFC6_sw_1_0_s_0_7.json\n",
      "EX_SPEC_MATRIX_HFFC6_sw_1_0_s_1_0.json\n",
      "EX_SPEC_MATRIX_HFFC6_sw_1_0_s_1_2.json\n",
      "EX_SPEC_MATRIX_HFFC6_sw_1_2_s_0_7.json\n",
      "EX_SPEC_MATRIX_HFFC6_sw_1_2_s_1_0.json\n",
      "EX_SPEC_MATRIX_HFFC6_sw_1_2_s_1_2.json\n"
     ]
    }
   ],
   "source": [
    "# Compare contents of two folders recursively\n",
    "\n",
    "import os\n",
    "import filecmp\n",
    "\n",
    "def compare_directories(dir1, dir2):\n",
    "    # Differ files\n",
    "    differ_files = []\n",
    "\n",
    "    # Compare the directories\n",
    "    dir_cmp = filecmp.dircmp(dir1, dir2)\n",
    "\n",
    "    # Files that are different\n",
    "    differ_files.extend(dir_cmp.diff_files)\n",
    "    \n",
    "    # Files only present in one of the directories\n",
    "    if dir_cmp.left_only or dir_cmp.right_only:\n",
    "        differ_files.extend(dir_cmp.left_only)\n",
    "        differ_files.extend(dir_cmp.right_only)\n",
    "\n",
    "    # Recursively compare subdirectories\n",
    "    for sub_dir in dir_cmp.subdirs:\n",
    "        differ_files.extend(compare_directories(\n",
    "            os.path.join(dir1, sub_dir),\n",
    "            os.path.join(dir2, sub_dir)\n",
    "        ))\n",
    "        \n",
    "    return differ_files\n",
    "\n",
    "def main():\n",
    "    folder1 = '../data/gallery_p1/specs_copy'\n",
    "    folder2 = '../data/gallery_p1/specs'\n",
    "\n",
    "    # Compare the folders\n",
    "    diff_files = compare_directories(folder1, folder2)\n",
    "\n",
    "    # Print out the differing files\n",
    "    if diff_files:\n",
    "        print(\"Differences found in the following files:\")\n",
    "        for file in diff_files:\n",
    "            print(file)\n",
    "    else:\n",
    "        print(\"The folders are identical.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7909b870-72c2-4530-a10e-549569d2b327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied: gene_annotation_p_2_sw_1_0_s_1_0.json\n",
      "Copied: gene_annotation_p_0_sw_1_2_s_1_0.json\n",
      "Copied: gene_annotation_p_0_sw_0_7_s_1_0.json\n",
      "Copied: gene_annotation_p_4_sw_0_7_s_1_0.json\n",
      "Copied: gene_annotation_p_3_sw_1_0_s_1_0.json\n",
      "Copied: gene_annotation_p_1_sw_1_2_s_1_0.json\n",
      "Copied: gene_annotation_p_1_sw_0_7_s_1_0.json\n",
      "Copied: gene_annotation_p_4_sw_1_2_s_1_0.json\n",
      "Copied: gene_annotation_p_0_sw_1_0_s_1_0.json\n",
      "Copied: gene_annotation_p_2_sw_1_2_s_1_0.json\n",
      "Copied: gene_annotation_p_2_sw_0_7_s_1_0.json\n",
      "Copied: gene_annotation_p_1_sw_1_0_s_1_0.json\n",
      "Copied: gene_annotation_p_3_sw_1_2_s_1_0.json\n",
      "Copied: gene_annotation_p_3_sw_0_7_s_1_0.json\n",
      "Copied: gene_annotation_p_4_sw_1_0_s_1_0.json\n",
      "Copied: gene_annotation_p_4_sw_1_0_s_0_7.json\n",
      "Copied: gene_annotation_p_2_sw_1_2_s_1_2.json\n",
      "Copied: gene_annotation_p_3_sw_0_7_s_0_7.json\n",
      "Copied: gene_annotation_p_0_sw_1_0_s_1_2.json\n",
      "Copied: gene_annotation_p_1_sw_1_0_s_0_7.json\n",
      "Copied: gene_annotation_p_3_sw_1_2_s_0_7.json\n",
      "Copied: gene_annotation_p_2_sw_0_7_s_1_2.json\n",
      "Copied: gene_annotation_p_3_sw_1_2_s_1_2.json\n",
      "Copied: gene_annotation_p_2_sw_0_7_s_0_7.json\n",
      "Copied: gene_annotation_p_1_sw_1_0_s_1_2.json\n",
      "Copied: gene_annotation_p_0_sw_1_0_s_0_7.json\n",
      "Copied: gene_annotation_p_2_sw_1_2_s_0_7.json\n",
      "Copied: gene_annotation_p_4_sw_1_0_s_1_2.json\n",
      "Copied: gene_annotation_p_3_sw_0_7_s_1_2.json\n",
      "Copied: gene_annotation_p_0_sw_1_2_s_1_2.json\n",
      "Copied: gene_annotation_p_1_sw_0_7_s_0_7.json\n",
      "Copied: gene_annotation_p_2_sw_1_0_s_1_2.json\n",
      "Copied: gene_annotation_p_4_sw_1_2_s_0_7.json\n",
      "Copied: gene_annotation_p_3_sw_1_0_s_0_7.json\n",
      "Copied: gene_annotation_p_4_sw_0_7_s_0_7.json\n",
      "Copied: gene_annotation_p_1_sw_1_2_s_0_7.json\n",
      "Copied: gene_annotation_p_0_sw_0_7_s_1_2.json\n",
      "Copied: gene_annotation_p_1_sw_1_2_s_1_2.json\n",
      "Copied: gene_annotation_p_0_sw_0_7_s_0_7.json\n",
      "Copied: gene_annotation_p_3_sw_1_0_s_1_2.json\n",
      "Copied: gene_annotation_p_4_sw_0_7_s_1_2.json\n",
      "Copied: gene_annotation_p_4_sw_1_2_s_1_2.json\n",
      "Copied: gene_annotation_p_2_sw_1_0_s_0_7.json\n",
      "Copied: gene_annotation_p_0_sw_1_2_s_0_7.json\n",
      "Copied: gene_annotation_p_1_sw_0_7_s_1_2.json\n",
      "Finished copying files.\n"
     ]
    }
   ],
   "source": [
    "# From a set of images (folder B), find their specs from a pool (A), and put them into folder C\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the paths to the folders\n",
    "folder_a_path = '../data/autogosling/specs'\n",
    "folder_b_path = './gene_annotation'\n",
    "folder_c_path = './gene_specs'\n",
    "\n",
    "# Ensure folder C exists\n",
    "os.makedirs(folder_c_path, exist_ok=True)\n",
    "\n",
    "# Get a set of all file names (without extension) in folder B\n",
    "images_in_b = {os.path.splitext(filename)[0] for filename in os.listdir(folder_b_path) if filename.endswith('.png')}\n",
    "\n",
    "# Iterate through each file in folder A\n",
    "for filename in os.listdir(folder_a_path):\n",
    "    # Split the filename to get the name and extension\n",
    "    name, extension = os.path.splitext(filename)\n",
    "    \n",
    "    # Check if the file is a JSON and if the corresponding PNG exists in folder B\n",
    "    if extension.lower() == '.json' and name in images_in_b:\n",
    "        # Construct full file paths\n",
    "        src_file_path = os.path.join(folder_a_path, filename)\n",
    "        dest_file_path = os.path.join(folder_c_path, filename)\n",
    "        \n",
    "        # Copy the file from A to C\n",
    "        shutil.copy2(src_file_path, dest_file_path)\n",
    "        print(f\"Copied: {filename}\")\n",
    "\n",
    "print(\"Finished copying files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5825c957-592d-463f-97ee-e8e9459665be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created EX_SPEC_MATRIX_HFFC6_sw_1_0_s_1_2_recolor.json from EX_SPEC_MATRIX_HFFC6_sw_1_0_s_1_2.json\n",
      "Created EX_SPEC_MATRIX_HFFC6_sw_1_0_s_0_7_recolor.json from EX_SPEC_MATRIX_HFFC6_sw_1_0_s_0_7.json\n",
      "Created EX_SPEC_MATRIX_HFFC6_sw_0_7_s_1_2_recolor.json from EX_SPEC_MATRIX_HFFC6_sw_0_7_s_1_2.json\n",
      "Created EX_SPEC_MATRIX_HFFC6_sw_1_2_s_1_2_recolor.json from EX_SPEC_MATRIX_HFFC6_sw_1_2_s_1_2.json\n",
      "Created EX_SPEC_MATRIX_HFFC6_sw_1_2_s_0_7_recolor.json from EX_SPEC_MATRIX_HFFC6_sw_1_2_s_0_7.json\n",
      "Created EX_SPEC_MATRIX_HFFC6_sw_0_7_s_0_7_recolor.json from EX_SPEC_MATRIX_HFFC6_sw_0_7_s_0_7.json\n",
      "Created EX_SPEC_MATRIX_HFFC6_sw_0_7_s_1_0_recolor.json from EX_SPEC_MATRIX_HFFC6_sw_0_7_s_1_0.json\n",
      "Created EX_SPEC_MATRIX_HFFC6_sw_1_2_s_1_0_recolor.json from EX_SPEC_MATRIX_HFFC6_sw_1_2_s_1_0.json\n",
      "Created EX_SPEC_MATRIX_HFFC6_sw_1_0_s_1_0_recolor.json from EX_SPEC_MATRIX_HFFC6_sw_1_0_s_1_0.json\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import copy\n",
    "\n",
    "def process_json_recursively(data):\n",
    "    \"\"\"Recursively process JSON data to modify 'warm' to 'hot'\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        new_data = {}\n",
    "        for key, value in data.items():            \n",
    "            if key == \"value\" and value == \"#0072B2\":\n",
    "                new_data[key] = \"#E79F00\"\n",
    "            elif key == \"value\" and value == \"#E79F00\":\n",
    "                new_data[key] = \"darkgreen\"\n",
    "            elif key == \"value\" and value == \"darkgreen\":\n",
    "                new_data[key] = \"purple\"\n",
    "            else:\n",
    "                new_data[key] = process_json_recursively(value)\n",
    "        return new_data\n",
    "    elif isinstance(data, list):\n",
    "        return [process_json_recursively(item) for item in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Specify the directory containing the JSON files\n",
    "directory_path = './geranium_seeds'  # replace with your directory path\n",
    "output_path = './recolor'\n",
    "\n",
    "# Track if any changes were made\n",
    "changes_made = False\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Open and read the JSON file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Process the JSON data recursively\n",
    "        new_data = process_json_recursively(json_data)\n",
    "\n",
    "        # Check if any changes were made by comparing the original and processed data\n",
    "        if json.dumps(new_data) != json.dumps(json_data):\n",
    "            # Create a new filename for the modified JSON\n",
    "            new_filename = f\"{filename[:-5]}_recolor.json\"  # Remove .json part and append _hot.json\n",
    "            new_file_path = os.path.join(output_path, new_filename)\n",
    "\n",
    "            # Write the new JSON data to a file\n",
    "            with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "                json.dump(new_data, new_file, indent=4)\n",
    "\n",
    "            print(f\"Created {new_filename} from {filename}\")\n",
    "            changes_made = True\n",
    "\n",
    "if not changes_made:\n",
    "    print(\"No files needed modification.\")\n",
    "else:\n",
    "    print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9158387-91df-4e8b-9f90-960dce291a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

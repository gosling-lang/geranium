{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e89213-91ea-4213-a2f1-a26f77343907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model loaded on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04bd8c11-fbb0-4fad-8d9c-a7ba61fa07aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define input directories\n",
    "image_folder = \"../data/unified/imgs\"\n",
    "text_folder = \"../data/unified/alt\"\n",
    "spec_folder = \"../data/unified/specs\"\n",
    "output_folder = \"./embeddings\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "batch_size = 50  # Adjust batch size as needed\n",
    "context_length = 256\n",
    "\n",
    "def generate_text_embeddings(folder, output_name):\n",
    "    text_files = [f for f in os.listdir(folder) if f.lower().endswith(\".txt\") or f.lower().endswith(\".json\")]\n",
    "    text_embeddings = []\n",
    "    file_names = []\n",
    "    \n",
    "    for i in range(0, len(text_files), batch_size):\n",
    "        batch_files = text_files[i:i + batch_size]\n",
    "        texts = []\n",
    "        \n",
    "        for text_file in batch_files:\n",
    "            text_path = os.path.join(folder, text_file)\n",
    "            \n",
    "            with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                if text_file.lower().endswith(\".json\"):\n",
    "                    try:\n",
    "                        data = json.load(f)\n",
    "                        text_desc = json.dumps(data)  # Convert JSON object to a string\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Warning: Could not parse JSON in {text_file}, skipping.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    text_desc = f.read().strip()\n",
    "            \n",
    "            texts.append(text_desc)\n",
    "            file_names.append(text_file)\n",
    "        \n",
    "        text_input = tokenizer(texts, context_length=context_length).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_features = model.encode_text(text_input)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        text_embeddings.append(text_features.cpu().numpy())\n",
    "        print(f\"Processed {i + batch_size} of {len(text_files)} text files.\")\n",
    "    \n",
    "    save_embeddings(text_embeddings, file_names, output_name)\n",
    "\n",
    "# Function to generate image embeddings in batches\n",
    "def generate_image_embeddings(folder, output_name):\n",
    "    image_files = [f for f in os.listdir(folder) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "    image_embeddings = []\n",
    "    file_names = []\n",
    "    \n",
    "    for i in range(0, len(image_files), batch_size):\n",
    "        batch_files = image_files[i:i + batch_size]\n",
    "        images = []\n",
    "        \n",
    "        for image_file in batch_files:\n",
    "            image_path = os.path.join(folder, image_file)\n",
    "            image = preprocess(Image.open(image_path).convert(\"RGB\"))\n",
    "            images.append(image)\n",
    "            file_names.append(image_file)\n",
    "        \n",
    "        images_tensor = torch.stack(images).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(images_tensor)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        image_embeddings.append(image_features.cpu().numpy())\n",
    "        print(f\"Processed {i + batch_size} of {len(image_files)} image files.\")\n",
    "    \n",
    "    save_embeddings(image_embeddings, file_names, output_name)\n",
    "\n",
    "# Function to save embeddings as TSV and Parquet\n",
    "def save_embeddings(embeddings, file_names, output_name):\n",
    "    if not embeddings:\n",
    "        print(f\"No embeddings generated for {output_name}\")\n",
    "        return\n",
    "    \n",
    "    embeddings_np = np.vstack(embeddings)\n",
    "    df = pd.DataFrame(embeddings_np, columns=[f\"dim_{i}\" for i in range(embeddings_np.shape[1])])\n",
    "    df.insert(0, \"Filename\", file_names)\n",
    "    \n",
    "    tsv_path = os.path.join(output_folder, f\"{output_name}.tsv\")\n",
    "    parquet_path = os.path.join(output_folder, f\"{output_name}.parquet\")\n",
    "    \n",
    "    df.to_csv(tsv_path, sep=\"\\t\", index=False)\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "    \n",
    "    print(f\"Embeddings saved: {tsv_path} and {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c89a685-42b6-42d4-a431-c7d790af5f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text embeddings...\n",
      "Processed 50 of 3200 text files.\n",
      "Processed 100 of 3200 text files.\n",
      "Processed 150 of 3200 text files.\n",
      "Processed 200 of 3200 text files.\n",
      "Processed 250 of 3200 text files.\n",
      "Processed 300 of 3200 text files.\n",
      "Processed 350 of 3200 text files.\n",
      "Processed 400 of 3200 text files.\n",
      "Processed 450 of 3200 text files.\n",
      "Processed 500 of 3200 text files.\n",
      "Processed 550 of 3200 text files.\n",
      "Processed 600 of 3200 text files.\n",
      "Processed 650 of 3200 text files.\n",
      "Processed 700 of 3200 text files.\n",
      "Processed 750 of 3200 text files.\n",
      "Processed 800 of 3200 text files.\n",
      "Processed 850 of 3200 text files.\n",
      "Processed 900 of 3200 text files.\n",
      "Processed 950 of 3200 text files.\n",
      "Processed 1000 of 3200 text files.\n",
      "Processed 1050 of 3200 text files.\n",
      "Processed 1100 of 3200 text files.\n",
      "Processed 1150 of 3200 text files.\n",
      "Processed 1200 of 3200 text files.\n",
      "Processed 1250 of 3200 text files.\n",
      "Processed 1300 of 3200 text files.\n",
      "Processed 1350 of 3200 text files.\n",
      "Processed 1400 of 3200 text files.\n",
      "Processed 1450 of 3200 text files.\n",
      "Processed 1500 of 3200 text files.\n",
      "Processed 1550 of 3200 text files.\n",
      "Processed 1600 of 3200 text files.\n",
      "Processed 1650 of 3200 text files.\n",
      "Processed 1700 of 3200 text files.\n",
      "Processed 1750 of 3200 text files.\n",
      "Processed 1800 of 3200 text files.\n",
      "Processed 1850 of 3200 text files.\n",
      "Processed 1900 of 3200 text files.\n",
      "Processed 1950 of 3200 text files.\n",
      "Processed 2000 of 3200 text files.\n",
      "Processed 2050 of 3200 text files.\n",
      "Processed 2100 of 3200 text files.\n",
      "Processed 2150 of 3200 text files.\n",
      "Processed 2200 of 3200 text files.\n",
      "Processed 2250 of 3200 text files.\n",
      "Processed 2300 of 3200 text files.\n",
      "Processed 2350 of 3200 text files.\n",
      "Processed 2400 of 3200 text files.\n",
      "Processed 2450 of 3200 text files.\n",
      "Processed 2500 of 3200 text files.\n",
      "Processed 2550 of 3200 text files.\n",
      "Processed 2600 of 3200 text files.\n",
      "Processed 2650 of 3200 text files.\n",
      "Processed 2700 of 3200 text files.\n",
      "Processed 2750 of 3200 text files.\n",
      "Processed 2800 of 3200 text files.\n",
      "Processed 2850 of 3200 text files.\n",
      "Processed 2900 of 3200 text files.\n",
      "Processed 2950 of 3200 text files.\n",
      "Processed 3000 of 3200 text files.\n",
      "Processed 3050 of 3200 text files.\n",
      "Processed 3100 of 3200 text files.\n",
      "Processed 3150 of 3200 text files.\n",
      "Processed 3200 of 3200 text files.\n",
      "Embeddings saved: ./embeddings/text_embeddings.tsv and ./embeddings/text_embeddings.parquet\n",
      "Generating specification embeddings...\n",
      "Processed 50 of 3200 text files.\n",
      "Processed 100 of 3200 text files.\n",
      "Processed 150 of 3200 text files.\n",
      "Processed 200 of 3200 text files.\n",
      "Processed 250 of 3200 text files.\n",
      "Processed 300 of 3200 text files.\n",
      "Processed 350 of 3200 text files.\n",
      "Processed 400 of 3200 text files.\n",
      "Processed 450 of 3200 text files.\n",
      "Processed 500 of 3200 text files.\n",
      "Processed 550 of 3200 text files.\n",
      "Processed 600 of 3200 text files.\n",
      "Processed 650 of 3200 text files.\n",
      "Processed 700 of 3200 text files.\n",
      "Processed 750 of 3200 text files.\n",
      "Processed 800 of 3200 text files.\n",
      "Processed 850 of 3200 text files.\n",
      "Processed 900 of 3200 text files.\n",
      "Processed 950 of 3200 text files.\n",
      "Processed 1000 of 3200 text files.\n",
      "Processed 1050 of 3200 text files.\n",
      "Processed 1100 of 3200 text files.\n",
      "Processed 1150 of 3200 text files.\n",
      "Processed 1200 of 3200 text files.\n",
      "Processed 1250 of 3200 text files.\n",
      "Processed 1300 of 3200 text files.\n",
      "Processed 1350 of 3200 text files.\n",
      "Processed 1400 of 3200 text files.\n",
      "Processed 1450 of 3200 text files.\n",
      "Processed 1500 of 3200 text files.\n",
      "Processed 1550 of 3200 text files.\n",
      "Processed 1600 of 3200 text files.\n",
      "Processed 1650 of 3200 text files.\n",
      "Processed 1700 of 3200 text files.\n",
      "Processed 1750 of 3200 text files.\n",
      "Processed 1800 of 3200 text files.\n",
      "Processed 1850 of 3200 text files.\n",
      "Processed 1900 of 3200 text files.\n",
      "Processed 1950 of 3200 text files.\n",
      "Processed 2000 of 3200 text files.\n",
      "Processed 2050 of 3200 text files.\n",
      "Processed 2100 of 3200 text files.\n",
      "Processed 2150 of 3200 text files.\n",
      "Processed 2200 of 3200 text files.\n",
      "Processed 2250 of 3200 text files.\n",
      "Processed 2300 of 3200 text files.\n",
      "Processed 2350 of 3200 text files.\n",
      "Processed 2400 of 3200 text files.\n",
      "Processed 2450 of 3200 text files.\n",
      "Processed 2500 of 3200 text files.\n",
      "Processed 2550 of 3200 text files.\n",
      "Processed 2600 of 3200 text files.\n",
      "Processed 2650 of 3200 text files.\n",
      "Processed 2700 of 3200 text files.\n",
      "Processed 2750 of 3200 text files.\n",
      "Processed 2800 of 3200 text files.\n",
      "Processed 2850 of 3200 text files.\n",
      "Processed 2900 of 3200 text files.\n",
      "Processed 2950 of 3200 text files.\n",
      "Processed 3000 of 3200 text files.\n",
      "Processed 3050 of 3200 text files.\n",
      "Processed 3100 of 3200 text files.\n",
      "Processed 3150 of 3200 text files.\n",
      "Processed 3200 of 3200 text files.\n",
      "Embeddings saved: ./embeddings/spec_embeddings.tsv and ./embeddings/spec_embeddings.parquet\n",
      "Generating image embeddings...\n",
      "Processed 50 of 3200 image files.\n",
      "Processed 100 of 3200 image files.\n",
      "Processed 150 of 3200 image files.\n",
      "Processed 200 of 3200 image files.\n",
      "Processed 250 of 3200 image files.\n",
      "Processed 300 of 3200 image files.\n",
      "Processed 350 of 3200 image files.\n",
      "Processed 400 of 3200 image files.\n",
      "Processed 450 of 3200 image files.\n",
      "Processed 500 of 3200 image files.\n",
      "Processed 550 of 3200 image files.\n",
      "Processed 600 of 3200 image files.\n",
      "Processed 650 of 3200 image files.\n",
      "Processed 700 of 3200 image files.\n",
      "Processed 750 of 3200 image files.\n",
      "Processed 800 of 3200 image files.\n",
      "Processed 850 of 3200 image files.\n",
      "Processed 900 of 3200 image files.\n",
      "Processed 950 of 3200 image files.\n",
      "Processed 1000 of 3200 image files.\n",
      "Processed 1050 of 3200 image files.\n",
      "Processed 1100 of 3200 image files.\n",
      "Processed 1150 of 3200 image files.\n",
      "Processed 1200 of 3200 image files.\n",
      "Processed 1250 of 3200 image files.\n",
      "Processed 1300 of 3200 image files.\n",
      "Processed 1350 of 3200 image files.\n",
      "Processed 1400 of 3200 image files.\n",
      "Processed 1450 of 3200 image files.\n",
      "Processed 1500 of 3200 image files.\n",
      "Processed 1550 of 3200 image files.\n",
      "Processed 1600 of 3200 image files.\n",
      "Processed 1650 of 3200 image files.\n",
      "Processed 1700 of 3200 image files.\n",
      "Processed 1750 of 3200 image files.\n",
      "Processed 1800 of 3200 image files.\n",
      "Processed 1850 of 3200 image files.\n",
      "Processed 1900 of 3200 image files.\n",
      "Processed 1950 of 3200 image files.\n",
      "Processed 2000 of 3200 image files.\n",
      "Processed 2050 of 3200 image files.\n",
      "Processed 2100 of 3200 image files.\n",
      "Processed 2150 of 3200 image files.\n",
      "Processed 2200 of 3200 image files.\n",
      "Processed 2250 of 3200 image files.\n",
      "Processed 2300 of 3200 image files.\n",
      "Processed 2350 of 3200 image files.\n",
      "Processed 2400 of 3200 image files.\n",
      "Processed 2450 of 3200 image files.\n",
      "Processed 2500 of 3200 image files.\n",
      "Processed 2550 of 3200 image files.\n",
      "Processed 2600 of 3200 image files.\n",
      "Processed 2650 of 3200 image files.\n",
      "Processed 2700 of 3200 image files.\n",
      "Processed 2750 of 3200 image files.\n",
      "Processed 2800 of 3200 image files.\n",
      "Processed 2850 of 3200 image files.\n",
      "Processed 2900 of 3200 image files.\n",
      "Processed 2950 of 3200 image files.\n",
      "Processed 3000 of 3200 image files.\n",
      "Processed 3050 of 3200 image files.\n",
      "Processed 3100 of 3200 image files.\n",
      "Processed 3150 of 3200 image files.\n",
      "Processed 3200 of 3200 image files.\n",
      "Embeddings saved: ./embeddings/image_embeddings.tsv and ./embeddings/image_embeddings.parquet\n",
      "All embeddings generated and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "print(\"Generating text embeddings...\")\n",
    "generate_text_embeddings(text_folder, \"text_embeddings\")\n",
    "\n",
    "print(\"Generating specification embeddings...\")\n",
    "generate_text_embeddings(spec_folder, \"spec_embeddings\")\n",
    "\n",
    "print(\"Generating image embeddings...\")\n",
    "generate_image_embeddings(image_folder, \"image_embeddings\")\n",
    "\n",
    "print(\"All embeddings generated and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29cc4c2f-0fc8-419c-8e9f-e54ea2dd9907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Filename     dim_0     dim_1  \\\n",
      "0     EX_SPEC_ALIGNMENT_CHART_sw_1_2_s_0_7_cc_2.txt -0.018945  0.034008   \n",
      "1  BRCA-EU-fc8130e0-a8b4-d80d-e040-11ac0c483272.txt -0.012375 -0.020747   \n",
      "2               breast_cancer_sw_1_0_s_0_5_cc_0.txt -0.005983  0.016782   \n",
      "3                EX_SPEC_CIRCOS_sw_1_0_s_0_7_oc.txt -0.050138 -0.004010   \n",
      "4                 rule-mark_p_0_sw_1_2_s_0_7_oc.txt -0.013534  0.004531   \n",
      "\n",
      "      dim_2     dim_3     dim_4     dim_5     dim_6     dim_7     dim_8  ...  \\\n",
      "0 -0.007164  0.047764 -0.012338 -0.039841 -0.014265 -0.042667  0.077361  ...   \n",
      "1 -0.045096  0.018411  0.031258 -0.022743 -0.015150 -0.126027  0.024359  ...   \n",
      "2 -0.036115 -0.000103  0.019164 -0.004601 -0.040937 -0.090851  0.039501  ...   \n",
      "3 -0.044706  0.001753 -0.013668 -0.011940 -0.032503 -0.069678  0.001210  ...   \n",
      "4 -0.012634 -0.000794  0.016319 -0.029607  0.017443 -0.008724  0.016731  ...   \n",
      "\n",
      "    dim_502   dim_503   dim_504   dim_505   dim_506   dim_507   dim_508  \\\n",
      "0 -0.004681  0.018526  0.031013 -0.090590  0.003158 -0.015574  0.023664   \n",
      "1 -0.076506 -0.018439 -0.036481 -0.014859 -0.031758 -0.007933 -0.073641   \n",
      "2 -0.023141  0.013828  0.000763 -0.023998 -0.015447 -0.010645 -0.016530   \n",
      "3 -0.047098  0.004937 -0.016580 -0.013110 -0.020956  0.009510 -0.036829   \n",
      "4 -0.012145  0.015862  0.014717 -0.055723 -0.001360 -0.009642  0.040478   \n",
      "\n",
      "    dim_509   dim_510   dim_511  \n",
      "0  0.034361 -0.057876  0.007996  \n",
      "1  0.002051 -0.051522 -0.002808  \n",
      "2  0.028792 -0.026635  0.038273  \n",
      "3 -0.035918 -0.040641 -0.034380  \n",
      "4  0.051412  0.007756  0.052370  \n",
      "\n",
      "[5 rows x 513 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the TSV file\n",
    "tsv_path = \"./embeddings/text_embeddings.tsv\"\n",
    "df_tsv = pd.read_csv(tsv_path, sep=\"\\t\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_tsv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb542b85-585d-4ff3-b958-a64005319888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Filename     dim_0     dim_1  \\\n",
      "0       EX_SPEC_CIRCULR_RANGE_sw_0_7_s_0_7_cc_0.json -0.001649 -0.064245   \n",
      "1             EX_SPEC_GREMLIN_sw_1_2_s_0_7_cc_1.json -0.021860 -0.027158   \n",
      "2  gray_heatmap_sw_1_0_s_1_0_oc_sw_0_7_s_1_0_cc_0...  0.009591 -0.089052   \n",
      "3                        TEXT_sw_0_7_s_1_0_cc_0.json  0.037189 -0.026111   \n",
      "4  PBCA-DE-2009e5e7-1796-445b-8677-46b3804fe0bf.json  0.027299 -0.008137   \n",
      "\n",
      "      dim_2     dim_3     dim_4     dim_5     dim_6     dim_7     dim_8  ...  \\\n",
      "0 -0.095751  0.007678  0.042844 -0.061872 -0.043238 -0.023653 -0.016709  ...   \n",
      "1 -0.045956 -0.003640 -0.009432 -0.034230 -0.024108 -0.050020  0.005480  ...   \n",
      "2 -0.061562  0.053112  0.019418 -0.013294 -0.009502 -0.045159  0.033293  ...   \n",
      "3 -0.063490  0.049418  0.078321 -0.031790 -0.013515 -0.027990  0.014375  ...   \n",
      "4 -0.057222 -0.018383 -0.002513 -0.033442  0.006589 -0.044671  0.046795  ...   \n",
      "\n",
      "    dim_502   dim_503   dim_504   dim_505   dim_506   dim_507   dim_508  \\\n",
      "0 -0.094069 -0.034622  0.027053 -0.069794 -0.023388  0.017606  0.010526   \n",
      "1 -0.080530 -0.018149  0.000750 -0.083066 -0.046891  0.023278  0.036841   \n",
      "2 -0.027075 -0.019355  0.059279 -0.075134 -0.054718  0.041852  0.073163   \n",
      "3 -0.051728 -0.038821  0.031918 -0.082526 -0.030518  0.013590  0.082175   \n",
      "4  0.002292 -0.025224 -0.029276 -0.042040 -0.081345 -0.035963  0.026293   \n",
      "\n",
      "    dim_509   dim_510   dim_511  \n",
      "0  0.022775 -0.071577  0.041331  \n",
      "1 -0.002192 -0.024872  0.020013  \n",
      "2 -0.007434 -0.015085  0.083667  \n",
      "3  0.001201 -0.015263  0.096938  \n",
      "4 -0.003124 -0.020417  0.013186  \n",
      "\n",
      "[5 rows x 513 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the Parquet file\n",
    "parquet_path = \"./embeddings/spec_embeddings.parquet\"\n",
    "df_parquet = pd.read_parquet(parquet_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df_parquet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4db0f2-483c-4983-8693-f512b8eb5327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
